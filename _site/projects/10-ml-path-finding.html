<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by yousinix
  Free for personal and commercial use under the MIT license
  https://github.com/yousinix/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Obstacle Detection from RGB Video Using a Neural Network">
  <meta property="og:description" content="Created a neural network for detecting obstacles using an RGB camera mounted on a mobile robot.">
  <meta property="og:image" content="https://scferro.github.io/assets/dl_out_sample6.png">

  <title>Obstacle Detection from RGB Video Using a Neural Network</title>
  <meta name="description" content="Created a neural network for detecting obstacles using an RGB camera mounted on a mobile robot.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>STEPHEN FERRO</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      <a class="nav-item nav-link " href="/resume/">Resume</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1 id="image-segmentation-and-obstacle-detection-from-rgb-video-using-a-neural-network">Image Segmentation and Obstacle Detection from RGB Video Using a Neural Network</h1>
<p><br /></p>

<h3 id="overview">Overview</h3>
<p>In this project, I worked with two other MSR students to build an neural network using Pytorch to detect obstacles in the path of a mobile robot. I then deployed the trained network on the <a href="https://scferro.github.io/projects/00-autonomous-rc-car" target="_blank"><u>Autonomous RC Car</u></a> I built to test it. Currently, I am working on using the trained network to further improve the lane centering abilities of the autonomous car while navigating through hallways, classrooms, and labs.</p>

<h3 id="video-demo">Video Demo</h3>
<p>Below is sample video captured while testing the network on the autonomous car robot. In this video, “safe” areas are marked in black, obstacles and unsafe areas are marked in white. In this video, it is clear that the robot is able to distinguish between walls and floors. It also seems to be able to detect some obstacles, like shoes of someone walking in front of it, but it struggles with other obstacles that were not in the training dataset. This will likely be improved with a more robust and diverse training dataset. More training time could improve these issues as well.</p>

<iframe width="800" height="450" src="https://www.youtube.com/embed/lt4EdjW5U3k?si=-NDz__lFQDcUUGZ5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<p><br /></p>

<h3 id="neural-network-architecture">Neural Network Architecture</h3>
<p>The architecture of this neural network is based on the deep convolutional neural network <a href="https://arxiv.org/abs/1511.00561v3" target="_blank"><u>SegNet, proposed by Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla</u></a> with some changes to better suit this application. This network is designed to map RGB images to segmented masks.</p>

<p><img src="http://localhost:4000/assets/cnn_paper.png" width="400" /></p>

<p>In our model, each encoder block includes the following layers:</p>
<ul>
  <li>Convolution layer 1 (Conv2d)</li>
  <li>Convolution layer 2 (Conv2d)</li>
  <li>Batch Normalization layer (BatchNorm2d)</li>
  <li>Non-Linear Activation layer (ReLU)</li>
  <li>Pooling layer (MaxPool)</li>
</ul>

<p>And each decoder block includes:</p>
<ul>
  <li>Upsampling layer (UpsamplingNearest2d)</li>
  <li>Convolution layer 1 (Conv2d)</li>
  <li>Convolution layer 2 (Conv2d)</li>
  <li>Batch Normalization layer (BatchNorm2d)</li>
  <li>Non-Linear Activation layer (ReLU)</li>
</ul>

<p>Compared to the architecture proposed in the paper, this model uses one less block for encoding and decoding. Additionally, we use only two convolutional layer in each block, while the paper uses three. We are only doing binary image segmentation, while the paper segments images into several regions. For this application, the extra complexity is not needed, and the simpler model trained more quickly and was easier to iterate on.</p>

<p>Below is the full model architecture, as output by the torchsummary Python library for a 240x240 image.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [16, 32, 240, 240]           2,432
            Conv2d-2         [16, 32, 240, 240]          25,632
    BatchNorm2d-3         [16, 32, 240, 240]              64
            ReLU-4         [16, 32, 240, 240]               0
        MaxPool2d-5         [16, 32, 120, 120]               0
            Conv2d-6         [16, 32, 120, 120]          25,632
            Conv2d-7         [16, 32, 120, 120]          25,632
    BatchNorm2d-8         [16, 32, 120, 120]              64
            ReLU-9         [16, 32, 120, 120]               0
        MaxPool2d-10           [16, 32, 60, 60]               0
        Conv2d-11           [16, 64, 60, 60]          51,264
        Conv2d-12           [16, 64, 60, 60]         102,464
    BatchNorm2d-13           [16, 64, 60, 60]             128
            ReLU-14           [16, 64, 60, 60]               0
        MaxPool2d-15           [16, 64, 30, 30]               0
        Conv2d-16          [16, 128, 30, 30]         204,928
        Conv2d-17          [16, 128, 30, 30]         409,728
    BatchNorm2d-18          [16, 128, 30, 30]             256
            ReLU-19          [16, 128, 30, 30]               0
        MaxPool2d-20          [16, 128, 15, 15]               0
UpsamplingNearest2d-21          [16, 128, 30, 30]               0
        Conv2d-22           [16, 64, 30, 30]         204,864
        Conv2d-23           [16, 64, 30, 30]         102,464
    BatchNorm2d-24           [16, 64, 30, 30]             128
            ReLU-25           [16, 64, 30, 30]               0
UpsamplingNearest2d-26          [16, 128, 60, 60]               0
        Conv2d-27           [16, 32, 60, 60]         102,432
        Conv2d-28           [16, 32, 60, 60]          25,632
    BatchNorm2d-29           [16, 32, 60, 60]              64
            ReLU-30           [16, 32, 60, 60]               0
UpsamplingNearest2d-31         [16, 64, 120, 120]               0
        Conv2d-32         [16, 32, 120, 120]          51,232
        Conv2d-33         [16, 32, 120, 120]          25,632
    BatchNorm2d-34         [16, 32, 120, 120]              64
            ReLU-35         [16, 32, 120, 120]               0
UpsamplingNearest2d-36         [16, 64, 240, 240]               0
        Conv2d-37          [16, 3, 240, 240]           4,803
        Conv2d-38          [16, 2, 240, 240]             152
    BatchNorm2d-39          [16, 2, 240, 240]               4
            ReLU-40          [16, 2, 240, 240]               0
================================================================
Total params: 1,365,695
Trainable params: 1,365,695
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 10.55
Forward/backward pass size (MB): 2380.08
Params size (MB): 5.21
Estimated Total Size (MB): 2395.83
----------------------------------------------------------------
</code></pre></div></div>

<p><br /></p>

<h3 id="dataset-creation">Dataset Creation</h3>
<p>To create the dataset, we drove the robot through hallways and labs using tele-op control while collecting and saving images from the camera. We then used the labeling tool <a href="https://labelstud.io/" target="_blank"><u>LabelStudio</u></a> to mark the “safe” regions for the robot to travel to in a selection of images.</p>

<p><img src="http://localhost:4000/assets/deep_learning_dataset.png" width="500" /></p>

<p>Once the initial dataset was created, we also augmented the dataset by duplicating and modifying the original images. For each original image, we did the following:</p>

<ul>
  <li>Crop each photo three times (shift left, centered, shift right)</li>
  <li>Created a mirrored copy of all images</li>
  <li>Copy each image and randomly adjust the HSV values</li>
  <li>Copy each images and add random white noise</li>
</ul>

<p>Using these methods, we increased our initial dataset of ~300 labelled images to &gt;7000. This made our training data much more robust and led to more reliable results.</p>

<p>Below are examples of a training image:</p>

<p><img src="http://localhost:4000/assets/dl_train_sample1.png" /></p>

<p>And here it’s is binary image label, where black regions are “safe” and white indicates “unsafe”:</p>

<p><img src="http://localhost:4000/assets/dl_train_sample2.png" /></p>

<p><br /></p>

<h3 id="initial-results">Initial Results</h3>
<p>Below, you can see initial results from the deploying the trained dataset on the robot. When compared to the label image, the output mask had 97% accuracy - that is, 97% of the pixels matched the label. in scenarios like this, where the model is primarily identifying walls, it tends to work very reliably.</p>

<p><img src="http://localhost:4000/assets/dl_out_sample2_comb.png" /></p>

<p>In the image below, the robot is easily able to detect the wall in front of it, and is mostly able to detect the shoe. It does label a small part of the shoe as “safe” however, enough of the shoe is labelled correctly for the robot to safely navigate around it. This image had 94% accuracy using the same method as above.</p>

<p><img src="http://localhost:4000/assets/dl_out_sample1_comb.png" /></p>

<h2 id="this-project-is-still-in-progress-check-back-soon-for-more">This project is still in progress, check back soon for more!</h2>

<p class="text-center">
<a class="m-1 btn btn-outline-primary btn-2 " href="https://github.com/scferro/deep_learning_hallway_detection">
  GitHub
</a>
</p>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Stephen Ferro</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:stephenferro2024@u.northwestern.edu"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/scferro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/scferro/"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.youtube.com//@StephenFerro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#ff0000'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-youtube fa-1x"></i>
    </a>

</div><small id="attribution">
    theme <a href="https://github.com/yousinix/portfolYOU">portfolYOU</a>
  </small>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>