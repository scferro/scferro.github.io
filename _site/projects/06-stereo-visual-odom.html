<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Real Time Stereo Visual Odometry">
  <meta property="og:description" content="Used stereo images to track the position of a camera in real time.">
  <meta property="og:image" content="https://scferro.github.io/assets/odom_main.gif">

  <title>Real Time Stereo Visual Odometry</title>
  <meta name="description" content="Used stereo images to track the position of a camera in real time.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>STEPHEN FERRO</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      <a class="nav-item nav-link " href="/resume/">Resume</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1 id="real-time-stereo-visual-odometry">Real Time Stereo Visual Odometry</h1>
<p><br /></p>

<h3 id="overview">Overview</h3>
<p>The primary goal of this project is to develop a real-time stereo visual odometry system that accurately estimates a robot’s motion using a stereo camera. By analyzing changes in captured images over time, the system aims to provide precise localization data crucial for the autonomous navigation of robots. This project focuses on implementing a robust methodology, including image processing, feature detection, and trajectory estimation, with an emphasis on evaluating three feature detection methods: SIFT, ORB, and the deep learning-based SuperPoint.</p>

<h3 id="video-demo">Video Demo</h3>
<iframe width="640" height="360" src="https://www.youtube.com/embed/hLsbvdzeaL0?si=GVzfOGvc_O5yF-40" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<p><br /></p>

<iframe width="640" height="360" src="https://www.youtube.com/embed/cD_iQAwCLJg?si=-AYiGhW3oUrKidc5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p><br /></p>

<h3 id="results">Results</h3>
<p><strong>KITTI Dataset Results:</strong></p>

<p><img src="http://localhost:4000/assets/kitti_traj.png" />
<br /></p>
<ol>
  <li><strong>Camera Trajectory:</strong> I plotted the estimated camera trajectories for each feature detection method (SIFT, ORB, and SuperPoint) alongside the ground truth from the KITTI dataset. This graph shows how accurately each method tracked the camera’s movement over time. SIFT and SuperPoint trajectories closely followed the ground truth, while ORB showed some deviation, especially in turns.
<br />
<img src="http://localhost:4000/assets/kitti_error.png" />
<br /></li>
  <li><strong>Trajectory Error:</strong> I calculated the cumulative error for each method over the course of the trajectory. SIFT and SuperPoint had similar error profiles, both performing better than ORB. The error increased over time for all methods, which is typical for visual odometry due to drift.
<br />
<img src="http://localhost:4000/assets/kitti_time.png" />
<br /></li>
  <li><strong>Frame Processing Times:</strong> This graph shows the average time taken to process each frame for the different methods. ORB was significantly faster, processing frames in about a quarter of the time needed for SIFT. SuperPoint was about twice as fast as SIFT but still slower than ORB.</li>
</ol>

<p><strong>RealSense Camera Results:</strong></p>

<p><img src="http://localhost:4000/assets/rs_traj.png" />
<br /></p>
<ol>
  <li><strong>Camera Trajectory:</strong> Similar to the KITTI results, I plotted the estimated trajectories for each method using data from the RealSense camera. In this real-world test, ORB performed surprisingly well, with a trajectory closer to the expected path than SIFT or SuperPoint.
<br />
<img src="http://localhost:4000/assets/rs_fr.png" />
<br /></li>
  <li><strong>Frames Processed Per Second:</strong> This graph shows the real-time performance of each method. ORB processed over 30 frames per second, significantly outperforming SIFT (7.5 fps) and SuperPoint (15 fps). This result emphasizes ORB’s suitability for real-time applications.</li>
</ol>

<p>These results show that while SIFT and SuperPoint were more accurate in controlled scenarios (KITTI dataset), ORB performed better in real-time, real-world conditions with the RealSense camera. This highlights the trade-off between accuracy and speed in visual odometry applications and demonstrates why testing with both datasets and real-world setups is crucial for a comprehensive evaluation.</p>

<h3 id="procedure">Procedure</h3>
<ol>
  <li><strong>Image Acquisition:</strong>
    <ul>
      <li>Load left and right images from stereo camera setup</li>
      <li>Use KITTI dataset for development and testing</li>
      <li>Utilize Intel RealSense d435i camera for real-time implementation</li>
    </ul>
  </li>
  <li><strong>Image Preprocessing:</strong>
    <ul>
      <li>Apply histogram normalization to both images using cv2.equalizeHist</li>
      <li>Enhance contrast across the entire image
Improve feature detection in varying lighting conditions</li>
      <li>Ensure uniform visibility across the image</li>
    </ul>
  </li>
  <li><strong>Disparity Computation:</strong>
    <ul>
      <li>Calculate disparity map using cv2.Stereo_SGBM algorithm</li>
      <li>Semi-Global Block Matching for improved accuracy</li>
      <li>Compute pixel-wise differences between left and right images</li>
      <li>Extract depth information from stereo image pairs</li>
      <li>Create a smooth disparity map for accurate 3D reconstruction</li>
    </ul>
  </li>
  <li><strong>Feature Detection:</strong>
    <ul>
      <li>Implement and compare three methods:</li>
      <li>SIFT (Scale-Invariant Feature Transform)</li>
      <li>ORB (Oriented FAST and Rotated BRIEF)</li>
      <li>SuperPoint (deep learning-based technique)</li>
    </ul>
  </li>
  <li><strong>Feature Matching:</strong>
    <ul>
      <li>Use brute force matching techniques</li>
      <li>Apply cv2.NORM_L2 for SIFT and SuperPoint, which uses Euclidean distance for floating-point descriptors</li>
      <li>Apply cv2.NORM_HAMMING for ORB, which uses Hamming distance for binary descriptors</li>
      <li>Establish correspondences between features in stereo image pairs</li>
    </ul>
  </li>
  <li><strong>Match Filtering:</strong>
    <ul>
      <li>Use Lowe’s ratio test to filter the best matches</li>
      <li>Compare distances of best and second-best matches</li>
      <li>Use a threshold (typically 0.75) to determine reliable matches</li>
      <li>Eliminate false correspondences</li>
      <li>Increase confidence in correctness of matched features</li>
    </ul>
  </li>
  <li><strong>3D Position Calculation:</strong>
    <ul>
      <li>Calculate 3D positions of matched points using disparity map and camera calibration data</li>
      <li>Apply triangulation to determine spatial coordinates</li>
      <li>Compute depth (Z) using focal length, baseline, and disparity</li>
      <li>Calculate X and Y coordinates in the camera frame</li>
    </ul>
  </li>
  <li><strong>Transform Estimation:</strong>
    <ul>
      <li>Estimate transformation between consecutive frames</li>
      <li>Use cv2.solvePnPRansac to calculate rotation and translation vectors</li>
      <li>Solve Perspective-n-Point (PnP) problem</li>
      <li>Employ RANSAC algorithm for robustness against outliers</li>
      <li>Convert 3D points of previous frame to 2D points of current frame</li>
    </ul>
  </li>
  <li><strong>Camera Position Update:</strong>
    <ul>
      <li>Update current camera position based on estimated transform</li>
      <li>Add new position to stored trajectory</li>
      <li>Maintain a continuous path of the camera’s movement</li>
      <li>Repeat steps for each new frame to continuously track movement</li>
      <li>Update trajectory with each new frame processed</li>
    </ul>
  </li>
</ol>

<h3 id="feature-detection-methods">Feature Detection Methods</h3>
<p>The decision to test different feature detection methods in this stereo visual odometry project was driven by the need to balance accuracy, speed, and adaptability in real-time applications. The project evaluated three distinct feature detection methods: SIFT, ORB, and SuperPoint.</p>

<p>I compared these different methods was to determine which approach would be most suitable for real-time stereo visual odometry. SIFT was chosen for its robustness and ability to detect a large number of features, albeit at the cost of slower processing. ORB was selected as a faster alternative, potentially more suitable for real-time applications. SuperPoint was included to explore the capabilities of a modern, deep learning-based approach that could potentially be fine-tuned for specific use cases.</p>

<p>SIFT is a robust algorithm that’s great at finding distinctive features in images that don’t change much with scale, rotation, or lighting. It works by making a scale space representation of the image and finding key points that are stable across different scales. Then it calculates descriptors for these points based on local gradients. SIFT is super accurate and reliable, especially when there are big changes in viewpoint or lighting. It usually finds the most features of the three methods, which can lead to more accurate motion estimation. The downside is that it’s computationally intensive, making it the slowest of the three and not great for real-time applications where speed is crucial. Features detected by SIFT are shown below on an image from the KITTI dataset.</p>

<p><br />
<img src="http://localhost:4000/assets/features_sift.png" />
<br /></p>

<p>ORB is designed to be a faster alternative to SIFT. It combines the FAST keypoint detector with the BRIEF descriptor. FAST is used to find keypoints, and a modified version of BRIEF computes binary descriptors for these points. ORB is optimized for speed, making it way faster than SIFT and good for real-time use. It finds fewer features than SIFT but offers a nice balance of speed and performance. ORB works particularly well in situations where you need rapid processing, like real-time tracking or AR. It might be less accurate than SIFT when there are big changes in scale or rotation though. In my tests, ORB performed best overall with the RealSense camera in real-time, processing over 30 frames per second while still being reasonably accurate. Features detected by ORB are shown below.</p>

<p><br />
<img src="http://localhost:4000/assets/features_orb.png" />
<br /></p>

<p>SuperPoint is a deep learning-based method developed by Magic Leap. It uses a fully convolutional neural network to detect keypoints and compute their descriptors at the same time. The network has a shared encoder and two decoder branches - one for finding keypoints and another for computing descriptors. SuperPoint is trained in a self-supervised way on a huge dataset, so it learns robust features that work well in lots of different environments. One of the coolest things about SuperPoint is that you can fine-tune it on specific datasets to make it work even better for particular tasks. In terms of accuracy, SuperPoint is often similar to SIFT, finding more features than ORB but fewer than SIFT. The trade-off is that it needs more computational power than ORB, so it’s slower for real-time applications. In my project, SuperPoint showed potential but needed more optimization to be practical for real-time use - it was the second slowest and least accurate in the RealSense camera tests. Features detected by SuperPoint are shown below.</p>

<p><br />
<img src="http://localhost:4000/assets/features_superpoint.png" />
<br /></p>

<p>Each of these methods has its own strengths and weaknesses when it comes to accuracy, speed, and adaptability. SIFT gives you high accuracy but it’s slow, ORB processes things quickly with decent accuracy, and SuperPoint is a modern approach that can be tailored to specific applications but needs more computational resources.</p>

<h3 id="kitti-dataset">KITTI Dataset</h3>
<p>For developing and testing my visual odometry algorithm, I used the KITTI dataset, a popular benchmark in computer vision and robotics research. It provides high-quality data from real-world driving scenarios, including urban, rural, and highway environments. KITTI’s stereo image pairs were perfect for my stereo visual odometry work, capturing various driving conditions and lighting situations. This helped me test my algorithm’s robustness. The dataset also includes ground truth data, which I used to calculate errors in my odometry estimates. I focused on the black and white stereo images and ground truth data, along with the provided intrinsic camera parameters needed for odometry calculations. KITTI allowed me to fine-tune my algorithm in a controlled way before moving to real-time testing with the RealSense camera.</p>

<h3 id="intel-realsense">Intel Realsense</h3>
<p>For real-time testing of my visual odometry system, I used an Intel RealSense d435i stereo camera. This camera was a great choice because it provides high-resolution black and white images from a pair of stereo infrared cameras, which work well in all lighting conditions. These images are similar to those in the greyscale KITTI dataset, making it easy to transition from development to real-world testing. To test the system, I mounted the RealSense on the back of a laptop and carried it through different environments, tracking its position and orientation in real-time. This setup allowed me to evaluate how well each feature detection method (SIFT, ORB, and SuperPoint) performed in practical, real-world scenarios. The RealSense’s ability to capture stereo images at high frame rates was crucial for assessing the real-time capabilities of my algorithm, especially for the faster methods like ORB. This real-world testing phase was key in determining which method was most suitable for practical applications of stereo visual odometry.</p>

<p><br />
<img src="http://localhost:4000/assets/realsense_mount.JPEG" />
<br /></p>

<h3 id="conclusions-and-future-work">Conclusions and Future Work</h3>
<p>The results of this project showed a clear trade-off between accuracy and speed among these methods. While SIFT and SuperPoint demonstrated higher accuracy in controlled scenarios, ORB emerged as the most practical option for real-time applications due to its speed and reasonable accuracy. This highlights the importance of balancing computational efficiency with precision in visual odometry systems, especially for real-time use cases.</p>

<p><strong>Future Work:</strong></p>

<ul>
  <li>Test the system on low-power edge computing devices like Nvidia Jetson or Raspberry Pi to assess performance in resource-constrained environments</li>
  <li>Implement ORB and SIFT tracking in C++ to potentially improve processing speed</li>
  <li>Train the SuperPoint network on an application-specific dataset to enhance its performance for particular use cases</li>
  <li>Experiment with different resolutions on the RealSense camera to find an optimal balance between detail and processing speed</li>
</ul>

<p>These improvements would make the system more robust, efficient, and applicable to various real-world scenarios.</p>

<p class="text-center">
<a class="m-1 btn btn-outline-primary btn-2 " href="https://github.com/scferro/stereo_visual_odom">
  GitHub
</a>
</p>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Stephen Ferro</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:stephenferro2024@u.northwestern.edu"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/scferro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/scferro/"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.youtube.com//@StephenFerro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#ff0000'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-youtube fa-1x"></i>
    </a>

</div><small id="attribution">
    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>
  </small>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>