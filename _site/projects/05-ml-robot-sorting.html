<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by yousinix
  Free for personal and commercial use under the MIT license
  https://github.com/yousinix/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Robot Arm Sorting with Online Learning via Human Interaction">
  <meta property="og:description" content="Created ROS 2 packages to control a robot arm to sort blocks based on human gesture feedback and interaction.">
  <meta property="og:image" content="https://scferro.github.io/assets/blocks_main.gif">

  <title>Robot Arm Sorting with Online Learning via Human Interaction</title>
  <meta name="description" content="Created ROS 2 packages to control a robot arm to sort blocks based on human gesture feedback and interaction.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>STEPHEN FERRO</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      <a class="nav-item nav-link " href="/resume/">Resume</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1 id="robot-arm-sorting-with-online-learning-via-human-interaction">Robot Arm Sorting with Online Learning via Human Interaction</h1>
<p><br /></p>

<p>THIS POST IS A WORK IN PROGRESS AND WILL BE COMPLETED BY 12.12.2024</p>

<h3 id="video-demo">Video Demo</h3>
<p>DEMO VIDEO COMING SOON</p>

<h3 id="overview">Overview</h3>

<p>In this project, I developed a novel interactive learning system that enables a robot arm to learn sorting behaviors through natural human interaction. The system combines computer vision, online learning, and natural human interaction to create a robot that can learn and refine sorting criteria throughout operation.</p>

<p>At its core, the system utilizes a Franka Panda robotic arm equipped with a depth camera for precise block detection and manipulation. A separate camera monitors the workspace for human interventions, enabling natural corrections through physical demonstrations. Finally, a combined ECG/IMU sensor worm by the operator enables corrections to the robot’s behavior via gesture control.</p>

<p>The system architecture integrates three neural networks:</p>
<ol>
  <li>Complex sorting network for four-category block classification</li>
  <li>Binary gesture network for yes/no feedback validation</li>
  <li>Complex gesture network for direct category corrections</li>
</ol>

<p>This multi-network approach enables fluid human-robot interaction, where users can either confirm the robot’s decisions through gestures or physically move blocks to demonstrate correct categorization. The system continuously learns from these interactions, improving its sorting accuracy while maintaining stable performance. The project demonstrates how combining traditional robotic control with adaptive learning and natural interaction modes can create systems that efficiently learn from and collaborate with human operators.</p>

<h3 id="sorting-process-overview">Sorting Process Overview</h3>

<p>The full sorting procedure is summarized in the block diagram below.</p>

<p><img src="http://localhost:4000/assets/final_block_diagram.png" /></p>
<h1 id="interactive-block-sorting-robot-system">Interactive Block-Sorting Robot System</h1>

<ol>
  <li>System Architecture
    <ul>
      <li>Uses ROS 2 framework with both C++ (robot control) and Python (ML/vision) nodes</li>
      <li>Integrates MoveIt for motion planning with the Franka Panda robot arm</li>
      <li>Uses depth cameras for visual perception</li>
      <li>Implements custom neural networks for both object sorting and gesture recognition</li>
    </ul>
  </li>
  <li>Main Control Flow
    <ul>
      <li>A sorting demonstration can optionally be given by the user to help pre-train the sorting network</li>
      <li>The sorting starts when the ‘sort_blocks’ action is called</li>
      <li>The robot moves to a scanning pose above the workspace</li>
      <li>A RealSense depth camera scans for blocks and creates 3D markers for detected objects</li>
      <li>Robot moves to scan the first detected block</li>
      <li>Sorting network predicts classification</li>
      <li>Robot grabs block and moves to wait position</li>
      <li>Gesture recognition checks for human agreement/disagreement</li>
      <li>Block is placed in appropriate pile based on final decision</li>
      <li>The process repeats until all blocks are sorted</li>
    </ul>
  </li>
  <li>Interactive Learning
    <ul>
      <li>Three neural networks work in parallel</li>
      <li>Sorting network: Classifies blocks based on visual features (4 categories)</li>
      <li>Binary Gesture network: Interprets human gestures for yes/no feedback</li>
      <li>Complex Gesture network: Interprets human gestures for sorting classifications (4 categories)</li>
      <li>Networks are continuously trained during operation</li>
    </ul>
  </li>
  <li>Human-Robot Interaction
    <ul>
      <li>Allows gesture-based corrections of sorting decisions</li>
      <li>Monitors for physical intervention (hand detection)</li>
      <li>Supports continuous learning from human feedback</li>
    </ul>
  </li>
</ol>

<p>The system combines autonomous operation with human oversight, enabling efficient sorting with continuous improvement through human feedback.</p>

<p>Below is a summary of the nodes used for this task:
<img src="http://localhost:4000/assets/final_node_summ.png" /></p>

<h3 id="neural-network-for-block-sorting">Neural Network for Block Sorting</h3>

<p>The system uses a sorting network built in PyTorch, designed for multi-category block classification. The network processes RGB images from the RealSense camera mounted on the robot’s end-effector and outputs classification decisions into four distinct categories.</p>

<p><strong>Network Architecture</strong>
The architecture consists of three convolutional layers with batch normalization and max pooling for feature extraction, followed by three fully connected layers. Key features include:</p>
<ul>
  <li>Input: 128x128 RGB images from RealSense camera</li>
  <li>Progressive feature extraction through three conv-pool stages</li>
  <li>Extensive regularization with batch normalization and dropout (0.5, 0.3)</li>
  <li>Four-category output using softmax activation</li>
  <li>Learning rate decay (γ=0.98) for stable online learning</li>
  <li>Cross-entropy loss with Adam optimizer</li>
</ul>

<p><strong>Online Learning System</strong></p>
<ul>
  <li>Maintains balanced class buffers (25 samples per category) which are filled in as the robot see and sorts more blocks</li>
  <li>Real-time training with performance logging through ROS 2 services</li>
  <li>State management for correction handling:
    <ul>
      <li>Caches network and buffer states before training</li>
      <li>Enables rollback for incorrect predictions</li>
      <li>Tracks confusion matrices and confidence metrics</li>
    </ul>
  </li>
</ul>

<p><strong>Pretraining by Demonstration</strong></p>

<p><img src="http://localhost:4000/assets/presorting.gif" /></p>

<p>The system provides a demonstration-based pretraining phase to establish initial sorting knowledge:</p>

<ul>
  <li>Initial Workspace Setup:
    <ul>
      <li>Blocks are arranged in predefined quadrants of the workspace</li>
      <li>Spatial locations define initial category labels:
        <ul>
          <li>Back left: Category 0</li>
          <li>Front left: Category 1</li>
          <li>Back right: Category 2</li>
          <li>Front right: Category 3</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Automated Learning Process:
    <ol>
      <li>Robot performs overhead scan to detect all blocks</li>
      <li>For each detected block:
        <ul>
          <li>Assigns category based on block’s quadrant location</li>
          <li>Creates training examples and updates category buffers</li>
          <li>Trains the sorting network as each image is added to the buffer</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>The pretraining phase provides the network with an initial understanding of visual features associated with each category, creating a higher baseline for further learning through interaction.</p>

<h3 id="gesture-feedback-sensor">Gesture Feedback Sensor</h3>
<p>Human-robot interactions in the project were facilitated by a wearable sensor created by Yayun Du of the Simpson Querrey Biomedical Research Center at Northwestern. This sensor features both a single channel ECG sensor and a 6-axis IMU to provide robust information about the behavior of the user. The sensor can be placed in a variety of locations on the body to sense different thing, from hand gestures to eye movements. For this application, it made the most sense to locate it on the wrist, where it would easily be able to return data about hand gestures, which could be used for guiding the robot in the sorting task.</p>

<p>Below are images showing the sensor itself and the positioning of the sensor on the arm. Once in the right position, the sensor is secured with adhesive pads to hold it in place.</p>

<p><img src="http://localhost:4000/assets/sensor.JPEG" /></p>

<p><img src="http://localhost:4000/assets/sensor_arm.JPEG" /></p>

<p>Two different types of gestures were used for this project. First, a yes/no binary gesture classification was used to confirm or correct a prediction made by the robot. I decided to make the “yes” gesture simply staying stationary, while the “no” gesture was making a cutting motion with your hand. This way, if the robot is sorting correctly, the user can simply stay in place and do nothing and the robot will proceed with sorting. The gif below shows examples of the two gestures.</p>

<p><img src="http://localhost:4000/assets/binary_gestures.gif" /></p>

<p>For getting sorting feedback, a four way gesture network (the “complex” gesture network) was used. The network itself is described in more detail below. The four gestures used are shown in the GIF below, and were each selected to take advantage of the specific sensing modalities of the sensor. These gestures each have very different ECG and IMU signals, due to the clenching and unclenching of the fist and the different directions of motion. This made classification very accurate and robust after training.</p>

<p><img src="http://localhost:4000/assets/complex_gestures.gif" /></p>

<p>The sensor communicates with the computer via Bluetooth. I created a ROS2 node to interface between the sensor and the ROS systems. The node publishes the sensors data to topics so it can be processed by the network_node for use in gesture predictions.</p>

<h3 id="neural-networks-for-gesture-control">Neural Networks for Gesture Control</h3>

<p>The system implements two complementary gesture networks for human feedback - a binary network for yes/no feedback and a complex network for direct category selection:</p>

<p><strong>Gesture Classification Data Processing</strong></p>
<ul>
  <li>Processes 4-channel time-series data from IMU sensors</li>
  <li>Sequence normalization using running maximum values</li>
  <li>Dynamic sequence padding/truncation to maintain 20-step sequences</li>
  <li>Feature concatenation from multiple sensor modalities for comprehensive gesture capture</li>
</ul>

<p><strong>Gesture Data Normalization</strong></p>
<ul>
  <li>Gesture data is normalized using maximum values collected using the <code class="language-plaintext highlighter-rouge">collect_norm_values</code> service</li>
  <li>This service collects data over a short period, then extracts and saves maximum values for normalization</li>
  <li>The user should clench their first and wave their arm around vigorously</li>
  <li>This normalization significantly increased the accuracy of gesture predictions</li>
</ul>

<p><strong>Binary Gesture Network (Yes/No)</strong></p>
<ul>
  <li>1D convolutional architecture optimized for temporal features</li>
  <li>Two convolutional layers with batch normalization and max pooling</li>
  <li>Three fully connected layers with dropout for robust classification</li>
  <li>Maintains 100-sample buffer for stable training</li>
  <li>Binary classification with BCE loss</li>
  <li>Higher initial learning rate (0.0005) for quick adaptation</li>
</ul>

<p><strong>Complex Gesture Network (Multi-Category)</strong></p>
<ul>
  <li>Enhanced temporal feature extraction with deeper architecture</li>
  <li>Larger convolutional layers (64 and 128 channels)</li>
  <li>Expanded fully connected layers for multi-class discrimination</li>
  <li>Four-category classification for direct sorting decisions</li>
  <li>Slower learning rate (0.00025) with gentler decay (γ=0.99)</li>
  <li>Enhanced buffer management for multi-class balance</li>
</ul>

<p><strong>Network Integration</strong>
Both networks operate in parallel during deployment:</p>
<ul>
  <li>Binary network provides quick validation of sorting decisions</li>
  <li>Complex network enables direct category corrections when needed</li>
  <li>Asynchronous service-based prediction and training for both networks</li>
  <li>Performance logging and analysis:
    <ul>
      <li>Tracks accuracy and confidence metrics</li>
      <li>Maintains confusion matrices</li>
      <li>Records correction statistics</li>
    </ul>
  </li>
  <li>Data collection for continuous improvement:
    <ul>
      <li>Saves training samples with timestamps</li>
      <li>Records sequence data for offline analysis</li>
      <li>Tracks model evolution through corrections</li>
    </ul>
  </li>
</ul>

<h3 id="franka-panda-control-with-moveit">Franka Panda Control with MoveIt</h3>

<p>The system integrates with MoveIt through the <code class="language-plaintext highlighter-rouge">MoveGroupInterface</code> and <code class="language-plaintext highlighter-rouge">PlanningSceneInterface</code>, which provide core functionality for motion planning and collision avoidance. Key custom functions were developed to handle common operations:</p>

<p><strong>Core Movement Functions:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">move_to_pose()</code>: Handles Cartesian space movement with configurable planning time and velocity/acceleration scaling</li>
  <li><code class="language-plaintext highlighter-rouge">move_to_joints()</code>: Provides direct joint space control for predefined configurations like the home position</li>
  <li><code class="language-plaintext highlighter-rouge">create_collision_box()</code> and <code class="language-plaintext highlighter-rouge">remove_collision_box()</code>: Manage collision objects for blocks and workspace boundaries</li>
  <li><code class="language-plaintext highlighter-rouge">scan_block()</code>: Executes a two-stage scanning motion sequence, first moving to a hover position above the target block, then to a precise scanning pose. This function coordinates with the vision system to update marker information and block dimensions.</li>
  <li><code class="language-plaintext highlighter-rouge">grab_block()</code>: Implements a multi-stage grasping operation that approaches blocks safely through hover and grasp poses. This function manages gripper control and updates collision tracking to reflect attached objects.</li>
  <li><code class="language-plaintext highlighter-rouge">place_in_stack()</code>: Handles the complete block placement sequence, managing stack heights, creating new stacks when needed, and coordinating collision object updates. The function ensures smooth transitions and proper spacing between blocks.</li>
  <li><code class="language-plaintext highlighter-rouge">place_block()</code>: Executes the final placement operation with precise position control and gripper release sequencing. This function includes retreat motions and updates marker positions to maintain accurate workspace representation.</li>
</ul>

<p>The system uses MoveIt’s planning scene to maintain a real-time representation of the workspace:</p>
<ul>
  <li>Tables and workspace boundaries are defined as permanent collision objects</li>
  <li>Dynamic collision objects are created for each block as it’s detected</li>
  <li>Objects are attached/detached from the gripper during manipulation</li>
  <li>Stack positions are tracked and updated to manage collision avoidance during placement</li>
</ul>

<p>All motion planning is then executed through MoveIt’s pipeline using the MoveIt C++ API.</p>

<h3 id="block-detection">Block Detection</h3>

<p><img src="http://localhost:4000/assets/scanning.gif" width="600" /></p>

<p>The block detection system employed computer vision techniques to accurately detect blocks, their dimensions, positions, and orientations. Key aspects included:</p>

<ol>
  <li>Camera Setup:
    <ul>
      <li>Utilized both color and depth information from a d405 RealSense camera mounted on the robot gripper. This camera is optimized for close range and high accuracy.</li>
      <li>To scan, the robot hand hovers above the workspace with the gripper and camera pointing straight down, where it can see the blocks.</li>
      <li>Depth data provided 3D positioning of the table in 3d space, while color data helps with block detection and segmentation.</li>
    </ul>
  </li>
  <li>Depth Segmentation:
    <ul>
      <li>Implemented a custom depth segmentation algorithm to isolate the table surface.</li>
      <li>Used Gaussian blur and Canny edge detection on the depth image.</li>
      <li>Applied Hough line transform to identify table edges.</li>
      <li>Created a mask to separate the table surface from the background.</li>
    </ul>
  </li>
  <li>Color-based Block Segmentation:
    <ul>
      <li>Applied color thresholding to isolate potential block regions.</li>
      <li>Used morphological operations (opening and closing) to reduce noise and refine block shapes.</li>
    </ul>
  </li>
  <li>Contour Analysis:
    <ul>
      <li>Detected contours in the segmented image using cv2.findContours().</li>
      <li>Analyzed contour properties (area, aspect ratio) to filter out non-block objects.</li>
      <li>Used cv2.minAreaRect() to find the oriented bounding box of each block.</li>
    </ul>
  </li>
  <li>3D Pose Estimation:
    <ul>
      <li>Calculated block orientation using the angle of the minimum area rectangle.</li>
      <li>Estimated block dimensions using the contour’s bounding rectangle and depth information.</li>
      <li>Depth data was used to estimate the height of the block.</li>
    </ul>
  </li>
</ol>

<h3 id="interaction-and-block-correction">Interaction and Block Correction</h3>

<p><img src="http://localhost:4000/assets/move_block.gif" width="600" /></p>

<p>The system continuously monitors the workspace to enable natural human interaction in the sorting process. This allows users to physically adjust block placements while the system learns from these corrections:</p>

<p><strong>Vision-Based Monitoring:</strong></p>
<ul>
  <li>Uses RealSense D435i camera for monitoring workspace state</li>
  <li>Continuous hand detection using MediaPipe hands detector</li>
  <li>Monitors predefined regions around:
    <ul>
      <li>Current block position</li>
      <li>All potential stack positions</li>
    </ul>
  </li>
  <li>Real-time depth change analysis for reliable interaction detection</li>
</ul>

<p><strong>Interaction Detection Pipeline:</strong></p>
<ol>
  <li><strong>Pre-Interaction State</strong>
    <ul>
      <li>Captures baseline color and depth images when hands enter workspace</li>
      <li>Tracks specific monitoring regions around current and potential block positions</li>
      <li>Uses depth thresholding to detect significant changes</li>
    </ul>
  </li>
  <li><strong>Movement Detection</strong>
    <ul>
      <li>Analyzes depth changes in monitored regions after hands leave</li>
      <li>Identifies block removal from original position</li>
      <li>Detects block addition in new stack locations</li>
      <li>Uses configurable thresholds for robust change detection</li>
    </ul>
  </li>
  <li><strong>State Updates</strong>
    <ul>
      <li>Updates internal block tracking based on detected movements</li>
      <li>Triggers network retraining based on human corrections</li>
      <li>Maintains consistency between physical state and system’s internal representation</li>
      <li>Updates all three networks when corrections occur using the correction services described above</li>
    </ul>
  </li>
</ol>

<p><strong>Network Corrections</strong></p>

<p>The network_node offers services to correct incorrect training each of the networks. These services reverse the incorrect training and retrain the network with the correct classification. This enables continuous learning from human feedback via physical interventions after the robot has already trained and updated the networks:</p>

<ul>
  <li>State Caching:
    <ul>
      <li>Each prediction caches a complete <code class="language-plaintext highlighter-rouge">NetworkState</code> including:
        <ul>
          <li>Model parameters (weights and biases)</li>
          <li>Optimizer state (momentum and adaptive learning rates)</li>
          <li>Learning rate scheduler configuration</li>
          <li>Training buffer contents for each category</li>
          <li>Original prediction and confidence values</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Correction Process:
    <ol>
      <li>System retrieves last training sample including cached state</li>
      <li>Removes incorrect example from its original category buffer</li>
      <li>Adds sample to correct category buffer with new label</li>
      <li>Reconstructs balanced training batch from all category buffers</li>
      <li>Retrains network using corrected and balanced data</li>
    </ol>
  </li>
</ul>

<p><strong>Camera Localization</strong></p>

<p><img src="http://localhost:4000/assets/localization.gif" width="600" /></p>

<p>The system uses AprilTag-based localization to accurately determine the D435 monitoring camera’s position in the workspace:</p>

<ul>
  <li>Multi-Stage Transform:</li>
  <li>Tracks AprilTag in both D405 (mounted on arm) and D435 frames</li>
  <li>Uses D405 tag detection to establish world-&gt;tag transform</li>
  <li>D435 provides tag-&gt;camera transform</li>
  <li>
    <p>Combines transforms to get precise world-&gt;D435 camera transform</p>
  </li>
  <li>Calibration Saving and Loading:</li>
  <li>Saves calibration to CSV file for persistent storage</li>
  <li>Loads previous calibration on startup if available</li>
  <li>Provides service interface for recalibration</li>
  <li>
    <p>Broadcasts world-&gt;d435_link transform continuously through TF system</p>
  </li>
  <li>Importance for Block Monitoring:</li>
  <li>Accurate camera position enables precise workspace monitoring</li>
  <li>Transforms block positions from world frame to camera frame</li>
  <li>Allows accurate region monitoring for block movements</li>
</ul>

<h3 id="network-training-and-evaluation">Network Training and Evaluation</h3>

<p><img src="http://localhost:4000/assets/pretrain_interface.png" /></p>

<p>The network_training node provides functionality for testing and training the networks without using the robot system:</p>

<ul>
  <li>Training Modes:
    <ul>
      <li>Sorting Only: Block classification only</li>
      <li>Gestures Only: Training gesture networks only</li>
      <li>Combined: Simultaneous training of two networks</li>
      <li>Supports both binary and complex gesture networks</li>
    </ul>
  </li>
  <li>Visual Interface:
    <ul>
      <li>Real-time display of training images</li>
      <li>Visual feedback of network predictions</li>
      <li>Confidence metrics for each classification</li>
      <li>Wait states for human feedback collection</li>
    </ul>
  </li>
  <li>Training Process:
    <ol>
      <li>System loads and displays random training image or waits for gesture</li>
      <li>Networks make initial predictions</li>
      <li>System displays predictions with confidence scores</li>
      <li>Human provides correct labels or confirms predictions</li>
      <li>Networks update based on feedback</li>
    </ol>
  </li>
</ul>

<p>This node serves as both a training and evaluation tool, allowing training sessions to improve network performance and evaluation of trained networks.</p>

<h3 id="online-training-results">Online Training Results</h3>

<p>The gesture networks were pretrained.</p>

<p>Binary Gestures:
<img src="http://localhost:4000/assets/merged_gesture_pretraining.png" /></p>

<p>Complex Gestures:
<img src="http://localhost:4000/assets/merged_complex_gesture_pretraining.png" /></p>

<p>Below is the online training of the solid color blocks. The first graph represents the outcomes of the video at the top of this post. The second set of graphs represent an extended training session using the network_training node.</p>

<p>Demo:
<img src="http://localhost:4000/assets/merged_sorting_colors_demo.png" /></p>

<p>Extended Training:
<img src="http://localhost:4000/assets/merged_sorting_colors_extended.png" /></p>

<p>After the network was trained, it could then be redeployed and retrained using a new sorting method. below are the results from redeploying the network in the demo in video and another from an extended training session in the network_training node. For comparison, I have also provided an example of an extended training session with no pretraining, showing that a pretrained network learns faster than the untrained network, even when using a different training method.</p>

<p>Extended Training with Pretraining:
<img src="http://localhost:4000/assets/merged_sorting_colors_patterns_extended_pretrained.png" /></p>

<p>Extended Training without Pretraining:
<img src="http://localhost:4000/assets/merged_sorting_colors_patterns_extended.png" /></p>

<p>Finally, the network can be deployed again to learn aother new training method.</p>

<p>Demo with Pretraining:
<img src="http://localhost:4000/assets/merged_sorting_patterns_demo.png" /></p>

<p>Extended Training with Pretraining:
<img src="http://localhost:4000/assets/merged_sorting_patterns_extended_pretrained.png" /></p>

<p>Extended Training without Pretraining:
<img src="http://localhost:4000/assets/merged_sorting_patterns_extended.png" /></p>

<h3 id="conclusion-and-future-work">Conclusion and Future Work</h3>

<p>This project successfully integrated robotics, computer vision, and machine learning to create an adaptive block sorting system. The system’s ability to learn and adapt to new sorting criteria, demonstrates potential for intelligent automation in industrial settings. Future work on this project could include:</p>

<ol>
  <li>Extending the system to handle a wider variety of objects with more complex shapes and materials, perhaps using an object detection model or vision-language-action model.</li>
  <li>Implementing more advanced reinforcement learning algorithms to further improve adaptability and efficiency.</li>
</ol>

<p>By combining adaptive learning with precise robotic control, this project laid the groundwork for more intelligent and versatile automation systems. The potential applications span various industries, from manufacturing and logistics to healthcare and beyond.</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>Thanks you to Matt Elwin and to Yayun Du for their guidance and support with this project!</p>

<p class="text-center">
<a class="m-1 btn btn-outline-primary btn-2 " href="https://github.com/scferro/franka_human_interaction">
  GitHub
</a>
</p>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Stephen Ferro</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:stephenferro2024@u.northwestern.edu"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/scferro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/scferro/"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.youtube.com//@StephenFerro"
       style="color: #6c757d"
       onMouseOver="this.style.color='#ff0000'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-youtube fa-1x"></i>
    </a>

</div><small id="attribution">
    theme <a href="https://github.com/yousinix/portfolYOU">portfolYOU</a>
  </small>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>